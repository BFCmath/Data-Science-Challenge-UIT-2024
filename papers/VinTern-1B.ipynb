{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a summary of what i can understand from the original paper.\n",
    "\n",
    "Link paper: [here](https://arxiv.org/pdf/2408.12480)\n",
    "\n",
    "Year: 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Overall Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " The Vintern-1B model is constructed using the InternVL-1.5 1 architecture, which is\n",
    " similar to the ’ViT-MLP-LLM’ configuration commonly found in many open-source MLLMs,\n",
    " as discussed in several studies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ **Vision Encoder**: InternViT-300M-448px a distilled small vision foundation model,\n",
    " is used as the vision encoder, which transform the image to visual features.\n",
    "+ **MLPProjector**: A two-layer Multi-Layer Perceptron (MLP) projector is utilized to map\n",
    " the output representations from both the visual and language modalities into a unified\n",
    " space, facilitating the integration of visual and textual information.\n",
    "+ **Large Language Model**: We utilize the pre-trained Qwen2-0.5B-Instruct serves\n",
    " as our language model, which generates text by leveraging the aligned representations\n",
    " provided by the MLP projector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![VinternArchitecture](images/VinternArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Implementation detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Vintern-1B employed a dynamic high-resolution approach, where images were divided\n",
    " into 448×448 pixel tiles. The number of tiles could vary,r eaching up to 12 based on the aspect\n",
    " ratio and resolution during training. In the testing phase,the model could handle as many as 12\n",
    " tiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We applied full-parameter fine-tuning to the Vision Encoder and MLP Projector. We used LoRA for the LLM. We leveraged the pre-trained InternVL2-1B model for visual instruction tuning to fully utilize the MLLM's capabilities across various multimodal tasks. For next-token prediction, we used cross-entropy loss, consistent with the approach during the pre-training stage.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
