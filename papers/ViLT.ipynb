{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This is just a summary of what i can understand from the original paper.\n",
    "\n",
    "Link paper: [here](https://arxiv.org/pdf/2102.03334)\n",
    "\n",
    "Year: 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To this date, most VLP studies have focused on improving\n",
    " performance by increasing the power of visual embedders.\n",
    " The shortcomings of having a heavy visual embedder are\n",
    " often disregarded in academic experiments because region\n",
    " features are commonly cached in advance at training time\n",
    " to ease the burden of feature extraction. However, the limitations are still evident in real-world applications as the\n",
    " queries in the wild have to undergo a slow extraction pro\n",
    "cess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To this end, we shift our attention to the lightweight and fast\n",
    " embedding of visual inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " This paper proposes the Vision-and-Language Transformer\n",
    " (ViLT) that handles two modalities in a single unified man\n",
    "ner. It mainly differs from previous VLP models in its\n",
    " shallow, convolution-free embedding of pixel-level inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ViLT is the simplest architecture by far for a vision\n",
    "and-language model as it commissions the transformer\n",
    " module to extract and process visual features in place\n",
    " of a separate deep visual embedder. This design in\n",
    "herently leads to significant runtime and parameter\n",
    " efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Vision-and-LanguageTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![ViLt](\\images\\ViLT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We deviate from the literature that we initialize the interaction transformer weights from pre-trained ViT instead of BERT. Such initialization exploits the power of the interaction layer to process visual features while lacking a separate deep visual embedder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![Fomula](images\\ViTfomula.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
