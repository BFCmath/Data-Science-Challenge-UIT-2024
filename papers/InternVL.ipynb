{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a summary of what i can understand from the original paper.\n",
    "\n",
    "Link paper: [here](https://arxiv.org/pdf/2312.14238)\n",
    "\n",
    "Year: 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To bridge vision models with LLMs, VLLMs commonly employ lightweight glue layers such as QFormer(BLIP 2) or linear projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Limitation:\n",
    "+ Disparity in params scales \n",
    "+ Inconsistent representation\n",
    "+ Inefficient connection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![InternVL](images/InternVL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The idea behind InternVL is that they formulate the InternVL, a\n",
    " large-scale vision-language foundation model, which aligns\n",
    " the representation of the scaled-up vision encoder with the\n",
    " LLM and achieves state-of-the-art performance on various\n",
    " visual and vision-language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To maintain the consistency of representations between the vision encoder and LLM, we employ a\n",
    " pre-trained multilingual LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Proposed method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Overal Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The proposed InternVL is designed with a vision encoder InternViT-6B and a language middleware QLLaMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " To align the two large-scale components with substan\n",
    "tial gaps in modalities and structures, we introduce a pro\n",
    "gressive alignment training strategy. The training strat\n",
    "egy is conducted progressively, beginning with contrastive\n",
    " learning on large-scale noisy data, and gradually moving\n",
    " towards generative learning on exquisite and high-quality data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![training method](images\\TrainingStrategyInternVL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Large-Scale Vision Encoder: InternViT-6B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ scale up vanila ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Language Middleware: QLLaMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The language mid\n",
    "dleware QLLaMA is proposed to align visual and linguistic features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Compared to recently popular approaches that use lightweight “glue” layers:\n",
    "+ By initializing with the pre-trained weights,  QLLaMA can transform im\n",
    "age tokens generated by InternViT-6B into the representa\n",
    "tion that is aligned with the LLMs;\n",
    "+ QLLaMA has 8 bil\n",
    "lion parameters for vision-language alignment, which are\n",
    " 42 times larger than the QFormer. Therefore, even with a\n",
    " frozen LLM decoder, InternVL can achieve promising per\n",
    "formance on multi-modal dialogue tasks. \n",
    "+ appliedtocontrastive learning, providingapowerful text\n",
    " representationforimage-textalignmenttasks,suchaszero\n",
    "shotimageclassificationandimage-textretrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**“Swiss Army Knife” Model: InternVL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "By flexibly combining the vision encoder and the language middleware, InternVL can support various vision or vision-language tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
