{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\uitds\n"
     ]
    }
   ],
   "source": [
    "%cd D:/uitds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test_file(name_file):\n",
    "    # the first row is the number of train_idx and val_idx\n",
    "    # the second row is the train_idx\n",
    "    # the third row is the val_idx\n",
    "    \n",
    "    with open(name_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        num_train, num_val = int(lines[0].split()[0]), int(lines[0].split()[1])\n",
    "        train_idx = list(map(int, lines[1].split()))\n",
    "        val_idx = list(map(int, lines[2].split()))\n",
    "    return num_train, num_val, train_idx, val_idx\n",
    "\n",
    "num_train, num_val, train_idx, val_idx = read_train_test_file('data\\\\train_val_group_split.txt')\n",
    "\n",
    "# read txt file: data\\train_val_group_split.txt\n",
    "TXT_PATH = \"data/train_val_group_split.txt\"\n",
    "\n",
    "def read_train_test_file(name_file):\n",
    "    \n",
    "    with open(name_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        num_train, num_val = int(lines[0].split()[0]), int(lines[0].split()[1])\n",
    "        train_idx = list(map(int, lines[1].split()))\n",
    "        val_idx = list(map(int, lines[2].split()))\n",
    "    return num_train, num_val, train_idx, val_idx\n",
    "\n",
    "num_train, num_val, train_idx, val_idx = read_train_test_file(TXT_PATH)\n",
    "\n",
    "JSON_PATH = \"data/json/vimmsd-train.json\"\n",
    "\n",
    "json_file = open(JSON_PATH, 'r')\n",
    "json_data = json.load(json_file)\n",
    "labels = [json_data[str(i)][\"label\"] for i in val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOREPEAT = \"data/json/vimmsd-train-no-repeat.json\"\n",
    "json_repeat_file = open(NOREPEAT, 'r')\n",
    "json_repeat_data = json.load(json_repeat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [] \n",
    "k = 0\n",
    "while k < len(val_idx):\n",
    "    # print(json_repeat_data[str(val_idx[k])][\"label\"])\n",
    "    groups += [k]*len(json_repeat_data[str(val_idx[k])][\"image\"])\n",
    "    k += len(json_repeat_data[str(val_idx[k])][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vintern_v1_epoch2_file = \"result_test_val/train_vintern_v1_epoch2.json\"\n",
    "vintern_v1_epoch3_file = \"result_test_val/train_vintern_v1_epoch3.json\"\n",
    "vintern_v1_epoch4_file = \"result_test_val/train_vintern_v1_epoch4.json\"\n",
    "vintern_v1_epoch5_file = \"result_test_val/train_vintern_v1_epoch5.json\"\n",
    "vintern_v1_1_epoch4_file = \"result_test_val/train_vintern_v1_1_epoch4.json\"\n",
    "vintern_v1_1_epoch5_file = \"result_test_val/train_vintern_v1_1_epoch5.json\"\n",
    "vintern_v1_1_epoch6_file = \"result_test_val/train_vintern_v1_1_epoch6.json\"\n",
    "vintern_v1_1_epoch7_file = \"result_test_val/train_vintern_v1_1_epoch7.json\"\n",
    "vintern_v2_epoch2_file = \"result_test_val/train_vintern_v2_epoch2.json\"\n",
    "vintern_v2_epoch3_file = \"result_test_val/train_vintern_v2_epoch3.json\"\n",
    "\n",
    "with open(vintern_v1_epoch2_file) as vintern_v1_epoch2, open(vintern_v1_epoch3_file) as vintern_v1_epoch3, \\\n",
    "     open(vintern_v1_epoch4_file) as vintern_v1_epoch4, open(vintern_v1_epoch5_file) as vintern_v1_epoch5, \\\n",
    "     open(vintern_v1_1_epoch4_file) as vintern_v1_1_epoch4, open(vintern_v1_1_epoch5_file) as vintern_v1_1_epoch5, \\\n",
    "     open(vintern_v1_1_epoch6_file) as vintern_v1_1_epoch6, open(vintern_v1_1_epoch7_file) as vintern_v1_1_epoch7, \\\n",
    "     open(vintern_v2_epoch2_file) as vintern_v2_epoch2, open(vintern_v2_epoch3_file) as vintern_v2_epoch3:\n",
    "\n",
    "    vintern_v1_epoch2 = json.load(vintern_v1_epoch2)\n",
    "    vintern_v1_epoch3 = json.load(vintern_v1_epoch3)\n",
    "    vintern_v1_epoch4 = json.load(vintern_v1_epoch4)\n",
    "    vintern_v1_epoch5 = json.load(vintern_v1_epoch5)\n",
    "    vintern_v1_1_epoch4 = json.load(vintern_v1_1_epoch4)\n",
    "    vintern_v1_1_epoch5 = json.load(vintern_v1_1_epoch5)\n",
    "    vintern_v1_1_epoch6 = json.load(vintern_v1_1_epoch6)\n",
    "    vintern_v1_1_epoch7 = json.load(vintern_v1_1_epoch7)\n",
    "    vintern_v2_epoch2 = json.load(vintern_v2_epoch2)\n",
    "    vintern_v2_epoch3 = json.load(vintern_v2_epoch3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vintern_v2_1_epoch3_file = \"result_test_val/train_vintern_v2_1_epoch3.json\"\n",
    "vintern_v2_1_epoch4_file = \"result_test_val/train_vintern_v2_1_epoch4.json\"\n",
    "vintern_v2_1_epoch5_file = \"result_test_val/train_vintern_v2_1_epoch5.json\"\n",
    "vintern_v3_epoch3_file = \"result_test_val/train_vintern_v3_epoch3.json\"\n",
    "vintern_v3_epoch4_file = \"result_test_val/train_vintern_v3_epoch4.json\"\n",
    "vintern_v5_epoch7_file = \"result_test_val/train_vintern_v5_epoch7.json\"\n",
    "vintern_v6_epoch2_file = \"result_test_val/train_vintern_v6_epoch2.json\"\n",
    "vintern_v6_epoch3_file = \"result_test_val/train_vintern_v6_epoch3.json\"\n",
    "vintern_v6_1_epoch2_file = \"result_test_val/train_vintern_v6_1_epoch2.json\"\n",
    "vintern_v6_1_epoch3_file = \"result_test_val/train_vintern_v6_1_epoch3.json\"\n",
    "\n",
    "with open(vintern_v2_1_epoch3_file) as vintern_v2_1_epoch3, open(vintern_v2_1_epoch4_file) as vintern_v2_1_epoch4, \\\n",
    "     open(vintern_v2_1_epoch5_file) as vintern_v2_1_epoch5, open(vintern_v3_epoch3_file) as vintern_v3_epoch3, \\\n",
    "     open(vintern_v3_epoch4_file) as vintern_v3_epoch4, open(vintern_v5_epoch7_file) as vintern_v5_epoch7, \\\n",
    "     open(vintern_v6_epoch2_file) as vintern_v6_epoch2, open(vintern_v6_epoch3_file) as vintern_v6_epoch3, \\\n",
    "     open(vintern_v6_1_epoch2_file) as vintern_v6_1_epoch2, open(vintern_v6_1_epoch3_file) as vintern_v6_1_epoch3:\n",
    "\n",
    "    vintern_v2_1_epoch3 = json.load(vintern_v2_1_epoch3)\n",
    "    vintern_v2_1_epoch4 = json.load(vintern_v2_1_epoch4)\n",
    "    vintern_v2_1_epoch5 = json.load(vintern_v2_1_epoch5)\n",
    "    vintern_v3_epoch3 = json.load(vintern_v3_epoch3)\n",
    "    vintern_v3_epoch4 = json.load(vintern_v3_epoch4)\n",
    "    vintern_v5_epoch7 = json.load(vintern_v5_epoch7)\n",
    "    vintern_v6_epoch2 = json.load(vintern_v6_epoch2)\n",
    "    vintern_v6_epoch3 = json.load(vintern_v6_epoch3)\n",
    "    vintern_v6_1_epoch2 = json.load(vintern_v6_1_epoch2)\n",
    "    vintern_v6_1_epoch3 = json.load(vintern_v6_1_epoch3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vintern_v6_1_epoch4_file = \"result_test_val/train_vintern_v6_1_epoch4.json\"\n",
    "yes_no_file = \"result_ova/Vintern_train_yes_no_ova_v1_epoch2.json\"\n",
    "yes_no_2_file = \"result_ova/Vintern_train_v4_2_epoch3.json\"\n",
    "vilt_1_file = \"result_test_val/train_vilt_classifier_WCE2_19_math.json\"\n",
    "vilt_2_file = \"result_test_val/train_vilt_embeddings_WCE2_4.json\"\n",
    "image_file = \"result_ova/Vintern_train_image_yes_no_ova_v1_epoch4.json\"\n",
    "text_file = \"result_ova/Vintern_train_text_ova_v1_epoch5.json\"\n",
    "\n",
    "with open(vintern_v6_1_epoch4_file) as vintern_v6_1_epoch4, open(yes_no_file) as yes_no, \\\n",
    "     open(yes_no_2_file) as yes_no_2, open(vilt_1_file) as vilt_1, open(vilt_2_file) as vilt_2, \\\n",
    "     open(image_file) as image, open(text_file) as text:\n",
    "\n",
    "    vintern_v6_1_epoch4 = json.load(vintern_v6_1_epoch4)\n",
    "    yes_no = json.load(yes_no)\n",
    "    yes_no_2 = json.load(yes_no_2)\n",
    "    vilt_1 = json.load(vilt_1)\n",
    "    vilt_2 = json.load(vilt_2)\n",
    "    image = json.load(image)\n",
    "    text = json.load(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    # 'v1_e2': vintern_v1_epoch2[\"results\"],\n",
    "    # 'v1_e3': vintern_v1_epoch3[\"results\"],\n",
    "    # 'v1_e4': vintern_v1_epoch4[\"results\"],\n",
    "    'v1_e5': vintern_v1_epoch5[\"results\"],\n",
    "    # 'v1_1_e4': vintern_v1_1_epoch4[\"results\"],\n",
    "    # 'v1_1_e5': vintern_v1_1_epoch5[\"results\"],\n",
    "    # 'v1_1_e6': vintern_v1_1_epoch6[\"results\"],\n",
    "    'v1_1_e7': vintern_v1_1_epoch7[\"results\"],\n",
    "    'v2_e2': vintern_v2_epoch2[\"results\"],\n",
    "    # 'v2_e3': vintern_v2_epoch3[\"results\"],\n",
    "    # 'v2_1_e3': vintern_v2_1_epoch3[\"results\"],\n",
    "    # 'v2_1_e4': vintern_v2_1_epoch4[\"results\"],\n",
    "    # 'v2_1_e5': vintern_v2_1_epoch5[\"results\"],\n",
    "    # 'v3_e3': vintern_v3_epoch3[\"results\"],\n",
    "    # 'v3_e4': vintern_v3_epoch4[\"results\"],\n",
    "    # 'v5_e7': vintern_v5_epoch7[\"results\"],\n",
    "    # 'v6_e2': vintern_v6_epoch2[\"results\"],\n",
    "    # 'v6_e3': vintern_v6_epoch3[\"results\"],\n",
    "    # 'v6_1_e2': vintern_v6_1_epoch2[\"results\"],\n",
    "    # 'v6_1_e3': vintern_v6_1_epoch3[\"results\"],\n",
    "    # 'v6_1_e4': vintern_v6_1_epoch4[\"results\"],\n",
    "    'yes_no': yes_no[\"results\"],\n",
    "    # 'yes_no_2': yes_no_2[\"results\"],\n",
    "    # 'vilt_1': vilt_1[\"results\"],\n",
    "    # 'vilt_2': vilt_2[\"results\"],\n",
    "    # 'image': image[\"results\"],\n",
    "    # 'text': text[\"results\"]\n",
    "})\n",
    "ohe_df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.418921090213773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'not-sarcasm': 854, 'multi-sarcasm': 697, 'image-sarcasm': 68})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# train model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight={\n",
    "        'not-sarcasm': 10,\n",
    "        'multi-sarcasm': 10,\n",
    "        'image-sarcasm': 100,\n",
    "        'text-sarcasm': 0\n",
    "    },\n",
    "    min_samples_split=20,\n",
    ")\n",
    "rf.fit(ohe_df, labels)\n",
    "\n",
    "# predict\n",
    "y_train_pred = rf.predict(ohe_df)\n",
    "\n",
    "# evaluate f1 macro\n",
    "print(f1_score(labels, y_train_pred, average='macro'))\n",
    "Counter(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'not-sarcasm': 947, 'multi-sarcasm': 599, 'image-sarcasm': 54, 'text-sarcasm': 19})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Initialize StratifiedGroupKFold with the desired number of splits\n",
    "stratified_group_kfold = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "# Perform the stratified group split\n",
    "for train_index, test_index in stratified_group_kfold.split(df, labels, groups):\n",
    "    train_df = df.iloc[train_index]\n",
    "    test_df = df.iloc[test_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    test_labels = [labels[i] for i in test_index]\n",
    "    \n",
    "train_df_ohe = pd.get_dummies(train_df)\n",
    "test_df_ohe = pd.get_dummies(test_df)\n",
    "\n",
    "# fill missing columns in test_df_ohe\n",
    "missing_cols = set(train_df_ohe.columns) - set(test_df_ohe.columns)\n",
    "\n",
    "for col in missing_cols:\n",
    "    test_df_ohe[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "0.42754259660411437\n",
      "0.4266327362461735\n",
      "Counter({'not-sarcasm': 189, 'multi-sarcasm': 120, 'image-sarcasm': 11, 'text-sarcasm': 3})\n",
      "Counter({'not-sarcasm': 175, 'multi-sarcasm': 142, 'image-sarcasm': 6})\n"
     ]
    }
   ],
   "source": [
    "# test many models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "    n_estimators=15,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    class_weight={\n",
    "        'not-sarcasm': 10,\n",
    "        'multi-sarcasm': 10,\n",
    "        'image-sarcasm': 100,\n",
    "        'text-sarcasm': 0\n",
    "    },\n",
    "    min_samples_split=10,\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    model.fit(train_df_ohe, train_labels)\n",
    "    y_pred = model.predict(test_df_ohe[train_df_ohe.columns])\n",
    "    print(f1_score(test_labels, y_pred, average='macro'))\n",
    "    y_pred_train = model.predict(train_df_ohe)\n",
    "    print(f1_score(train_labels,y_pred_train, average='macro'))\n",
    "    print(Counter(test_labels))\n",
    "    print(Counter(y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vintern_v1_epoch2_file = \"result_test_val/test_vintern_v1_epoch2.json\"\n",
    "vintern_v1_epoch3_file = \"result_test_val/test_vintern_v1_epoch3.json\"\n",
    "vintern_v1_epoch4_file = \"result_test_val/test_vintern_v1_epoch4.json\"\n",
    "vintern_v1_epoch5_file = \"result_test_val/test_vintern_v1_epoch5.json\"\n",
    "vintern_v1_1_epoch4_file = \"result_test_val/test_vintern_v1_1_epoch4.json\"\n",
    "vintern_v1_1_epoch5_file = \"result_test_val/test_vintern_v1_1_epoch5.json\"\n",
    "vintern_v1_1_epoch6_file = \"result_test_val/test_vintern_v1_1_epoch6.json\"\n",
    "vintern_v1_1_epoch7_file = \"result_test_val/test_vintern_v1_1_epoch7.json\"\n",
    "vintern_v2_epoch2_file = \"result_test_val/test_vintern_v2_epoch2.json\"\n",
    "vintern_v2_epoch3_file = \"result_test_val/test_vintern_v2_epoch3.json\"\n",
    "\n",
    "with open(vintern_v1_epoch2_file) as vintern_v1_epoch2, open(vintern_v1_epoch3_file) as vintern_v1_epoch3, \\\n",
    "    open(vintern_v1_epoch4_file) as vintern_v1_epoch4, open(vintern_v1_epoch5_file) as vintern_v1_epoch5, \\\n",
    "    open(vintern_v1_1_epoch4_file) as vintern_v1_1_epoch4, open(vintern_v1_1_epoch5_file) as vintern_v1_1_epoch5, \\\n",
    "    open(vintern_v1_1_epoch6_file) as vintern_v1_1_epoch6, open(vintern_v1_1_epoch7_file) as vintern_v1_1_epoch7, \\\n",
    "    open(vintern_v2_epoch2_file) as vintern_v2_epoch2, open(vintern_v2_epoch3_file) as vintern_v2_epoch3:\n",
    "\n",
    "    vintern_v1_epoch2 = json.load(vintern_v1_epoch2)\n",
    "    vintern_v1_epoch3 = json.load(vintern_v1_epoch3)\n",
    "    vintern_v1_epoch4 = json.load(vintern_v1_epoch4)\n",
    "    vintern_v1_epoch5 = json.load(vintern_v1_epoch5)\n",
    "    vintern_v1_1_epoch4 = json.load(vintern_v1_1_epoch4)\n",
    "    vintern_v1_1_epoch5 = json.load(vintern_v1_1_epoch5)\n",
    "    vintern_v1_1_epoch6 = json.load(vintern_v1_1_epoch6)\n",
    "    vintern_v1_1_epoch7 = json.load(vintern_v1_1_epoch7)\n",
    "    vintern_v2_epoch2 = json.load(vintern_v2_epoch2)\n",
    "    vintern_v2_epoch3 = json.load(vintern_v2_epoch3)\n",
    "\n",
    "\n",
    "vintern_v2_1_epoch3_file = \"result_test_val/test_vintern_v2_1_epoch3.json\"\n",
    "vintern_v2_1_epoch4_file = \"result_test_val/test_vintern_v2_1_epoch4.json\"\n",
    "vintern_v2_1_epoch5_file = \"result_test_val/test_vintern_v2_1_epoch5.json\"\n",
    "vintern_v3_epoch3_file = \"result_test_val/test_vintern_v3_epoch3.json\"\n",
    "vintern_v3_epoch4_file = \"result_test_val/test_vintern_v3_epoch4.json\"\n",
    "vintern_v5_epoch7_file = \"result_test_val/test_vintern_v5_epoch7.json\"\n",
    "vintern_v6_epoch2_file = \"result_test_val/test_vintern_v6_epoch2.json\"\n",
    "vintern_v6_epoch3_file = \"result_test_val/test_vintern_v6_epoch3.json\"\n",
    "vintern_v6_1_epoch2_file = \"result_test_val/test_vintern_v6_1_epoch2.json\"\n",
    "vintern_v6_1_epoch3_file = \"result_test_val/test_vintern_v6_1_epoch3.json\"\n",
    "\n",
    "with open(vintern_v2_1_epoch3_file) as vintern_v2_1_epoch3, open(vintern_v2_1_epoch4_file) as vintern_v2_1_epoch4, \\\n",
    "    open(vintern_v2_1_epoch5_file) as vintern_v2_1_epoch5, open(vintern_v3_epoch3_file) as vintern_v3_epoch3, \\\n",
    "    open(vintern_v3_epoch4_file) as vintern_v3_epoch4, open(vintern_v5_epoch7_file) as vintern_v5_epoch7, \\\n",
    "    open(vintern_v6_epoch2_file) as vintern_v6_epoch2, open(vintern_v6_epoch3_file) as vintern_v6_epoch3, \\\n",
    "    open(vintern_v6_1_epoch2_file) as vintern_v6_1_epoch2, open(vintern_v6_1_epoch3_file) as vintern_v6_1_epoch3:\n",
    "\n",
    "    vintern_v2_1_epoch3 = json.load(vintern_v2_1_epoch3)\n",
    "    vintern_v2_1_epoch4 = json.load(vintern_v2_1_epoch4)\n",
    "    vintern_v2_1_epoch5 = json.load(vintern_v2_1_epoch5)\n",
    "    vintern_v3_epoch3 = json.load(vintern_v3_epoch3)\n",
    "    vintern_v3_epoch4 = json.load(vintern_v3_epoch4)\n",
    "    vintern_v5_epoch7 = json.load(vintern_v5_epoch7)\n",
    "    vintern_v6_epoch2 = json.load(vintern_v6_epoch2)\n",
    "    vintern_v6_epoch3 = json.load(vintern_v6_epoch3)\n",
    "    vintern_v6_1_epoch2 = json.load(vintern_v6_1_epoch2)\n",
    "    vintern_v6_1_epoch3 = json.load(vintern_v6_1_epoch3)\n",
    "    \n",
    "vintern_v6_1_epoch4_file = \"result_test_val/test_vintern_v6_1_epoch4.json\"\n",
    "yes_no_file = \"result_ova/Vintern_test_yes_no_ova_v1_epoch2.json\"\n",
    "yes_no_2_file = \"result_ova/Vintern_test_v4_2_epoch3.json\"\n",
    "vilt_1_file = \"result_test_val/test_vilt_classifier_WCE2_19_math.json\"\n",
    "vilt_2_file = \"result_test_val/test_vilt_embeddings_WCE2_4.json\"\n",
    "image_file = \"result_ova/Vintern_test_image_yes_no_ova_v1_epoch4.json\"\n",
    "text_file = \"result_ova/Vintern_test_text_ova_v1_epoch5.json\"\n",
    "\n",
    "with open(vintern_v6_1_epoch4_file) as vintern_v6_1_epoch4, open(yes_no_file) as yes_no, \\\n",
    "    open(yes_no_2_file) as yes_no_2, open(vilt_1_file) as vilt_1, open(vilt_2_file) as vilt_2, \\\n",
    "    open(image_file) as image, open(text_file) as text:\n",
    "\n",
    "    vintern_v6_1_epoch4 = json.load(vintern_v6_1_epoch4)\n",
    "    yes_no = json.load(yes_no)\n",
    "    yes_no_2 = json.load(yes_no_2)\n",
    "    vilt_1 = json.load(vilt_1)\n",
    "    vilt_2 = json.load(vilt_2)\n",
    "    image = json.load(image)\n",
    "    text = json.load(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'v1_e2': vintern_v1_epoch2[\"results\"],\n",
    "    'v1_e3': vintern_v1_epoch3[\"results\"],\n",
    "    'v1_e4': vintern_v1_epoch4[\"results\"],\n",
    "    'v1_e5': vintern_v1_epoch5[\"results\"],\n",
    "    'v1_1_e4': vintern_v1_1_epoch4[\"results\"],\n",
    "    'v1_1_e5': vintern_v1_1_epoch5[\"results\"],\n",
    "    'v1_1_e6': vintern_v1_1_epoch6[\"results\"],\n",
    "    'v1_1_e7': vintern_v1_1_epoch7[\"results\"],\n",
    "    'v2_e2': vintern_v2_epoch2[\"results\"],\n",
    "    'v2_e3': vintern_v2_epoch3[\"results\"],\n",
    "    'v2_1_e3': vintern_v2_1_epoch3[\"results\"],\n",
    "    'v2_1_e4': vintern_v2_1_epoch4[\"results\"],\n",
    "    'v2_1_e5': vintern_v2_1_epoch5[\"results\"],\n",
    "    'v3_e3': vintern_v3_epoch3[\"results\"],\n",
    "    'v3_e4': vintern_v3_epoch4[\"results\"],\n",
    "    'v5_e7': vintern_v5_epoch7[\"results\"],\n",
    "    'v6_e2': vintern_v6_epoch2[\"results\"],\n",
    "    'v6_e3': vintern_v6_epoch3[\"results\"],\n",
    "    'v6_1_e2': vintern_v6_1_epoch2[\"results\"],\n",
    "    'v6_1_e3': vintern_v6_1_epoch3[\"results\"],\n",
    "    'v6_1_e4': vintern_v6_1_epoch4[\"results\"],\n",
    "    'yes_no': yes_no[\"results\"],\n",
    "    'yes_no_2': yes_no_2[\"results\"],\n",
    "    'vilt_1': vilt_1[\"results\"],\n",
    "    'vilt_2': vilt_2[\"results\"],\n",
    "    'image': image[\"results\"],\n",
    "    'text': text[\"results\"]\n",
    "})\n",
    "ohe_df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohe_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing columns in test_df_ohe\n",
    "missing_cols = set(train_df_ohe.columns) - set(ohe_df_test.columns)\n",
    "\n",
    "for col in missing_cols:\n",
    "    ohe_df_test[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'multi-sarcasm': 902, 'not-sarcasm': 462, 'image-sarcasm': 49})\n"
     ]
    }
   ],
   "source": [
    "test_pred = rf.predict(ohe_df_test[ohe_df.columns])\n",
    "from collections import Counter\n",
    "print(Counter(test_pred))\n",
    "results = {}\n",
    "results[\"results\"] = {}\n",
    "results[\"phase\"] = \"dev\"\n",
    "merge_test = test_pred\n",
    "for i in range(len(merge_test)):\n",
    "    results[\"results\"][str(i)] = merge_test[i]\n",
    "with open(f\"results.json\", \"w\") as out:\n",
    "    json.dump(results, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'multi-sarcasm': 995, 'not-sarcasm': 383, 'image-sarcasm': 35})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"merge_test/best_v6_epoch3_best2_v1_epoch5_not_v1_epoch2_results.json\"\n",
    "with open(file) as f:\n",
    "    data = json.load(f)\n",
    "    results = data[\"results\"]\n",
    "    \n",
    "Counter(results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
